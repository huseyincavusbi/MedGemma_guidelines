{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPgRPA-fUsP2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load the Model\n",
        "# Unsloth will automatically handle 4-bit quantization and PEFT configuration.\n",
        "# We also specify a max sequence length for the model.\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/medgemma-4b-pt\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    dtype = None, # Will default to torch.bfloat16 if available\n",
        ")\n",
        "\n",
        "# 2. Configure LoRA Adapters\n",
        "# This adds trainable \"adapter\" layers to the model.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank of the adapters. A common choice.\n",
        "    lora_alpha = 16, # A scaling factor for the adapters.\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 42,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        ")\n",
        "\n",
        "print(\"Unsloth model configured for 4-bit LoRA fine-tuning!\")"
      ],
      "metadata": {
        "id": "QktrYHxPUxlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load just the first example from the dataset to inspect its structure\n",
        "ds_preview = load_dataset(\"epfl-llm/guidelines\", split=\"train\", streaming=True).take(1)\n",
        "example = list(ds_preview)[0]\n",
        "\n",
        "print(\"Dataset Columns Found:\")\n",
        "print(example.keys())"
      ],
      "metadata": {
        "id": "Un6qdZdKXcLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CORRECTED STEP 2: Prepare the Dataset (Based on Actual Data Structure) ---\n",
        "\n",
        "# Define a simple prompt structure\n",
        "prompt_template = \"\"\"### Source:\n",
        "{}\n",
        "\n",
        "### Guideline Text:\n",
        "{}\"\"\"\n",
        "\n",
        "# We need a special token to signify the end of a sequence\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Function to format each example in the dataset\n",
        "def format_prompt(example):\n",
        "    # CORRECTED: Use 'clean_text' for the main content and 'source' for the origin.\n",
        "    # These keys are confirmed to be in the dataset from your debugging.\n",
        "    formatted_text = prompt_template.format(example['source'], example['clean_text']) + EOS_TOKEN\n",
        "\n",
        "    # Return a dictionary with a single key named \"text\",\n",
        "    # as this is what the SFTTrainer expects by default.\n",
        "    return { \"text\" : formatted_text }\n",
        "\n",
        "\n",
        "# Load the full dataset for training\n",
        "ds = load_dataset(\"epfl-llm/guidelines\", split=\"train\")\n",
        "\n",
        "# Apply the formatting function to the entire dataset\n",
        "# This will now work because we are using the correct, verified keys.\n",
        "ds = ds.map(format_prompt, num_proc=4) # Using multiple processes to speed it up\n",
        "\n",
        "print(\"\\nDataset loaded and formatted successfully!\")\n",
        "print(\"Here is an example of a formatted prompt:\")\n",
        "print(ds[0]['text']) # Print the first example to see the final format"
      ],
      "metadata": {
        "id": "gNTfITeLXm04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = ds,\n",
        "    dataset_text_field = \"text\", # The name of the field containing our formatted prompts\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can be set to True for faster training on many short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Simulates a larger batch size (2 * 4 = 8)\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 500, # A small number for demonstration. Increase for a full run (e.g., 200-500).\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(), # Use bf16 if available, else fp16\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\", # Use 8-bit optimizer to save memory\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 42,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Let's start the training!\n",
        "print(\"Starting the fine-tuning process...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"Fine-tuning complete!\")"
      ],
      "metadata": {
        "id": "0f2XUe0AVYln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test our newly fine-tuned model\n",
        "# We'll use the same prompt template, but only provide the \"Source\" part\n",
        "# The model should then complete the \"Guideline Text\" part\n",
        "\n",
        "# Load the base model and tokenizer for inference\n",
        "from transformers import pipeline\n",
        "\n",
        "# You can use the `trainer.model` directly if you're in the same session\n",
        "# Or load the saved adapters like this for a new session\n",
        "# from unsloth import FastLanguageModel\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"outputs/checkpoint-60\", # Or your final saved model\n",
        "# )\n",
        "\n",
        "text_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer)\n",
        "\n",
        "# Create a test prompt\n",
        "test_prompt_input = \"American College of Cardiology\"\n",
        "test_prompt_formatted = prompt_template.format(test_prompt_input, \"\") # The empty string is where the model will generate\n",
        "\n",
        "# Run inference\n",
        "output = text_pipeline(test_prompt_formatted, max_new_tokens=256)\n",
        "\n",
        "# Print the result\n",
        "print(\"=\"*50)\n",
        "print(\"PROMPT:\")\n",
        "print(test_prompt_formatted)\n",
        "print(\"\\nMODEL OUTPUT:\")\n",
        "print(output[0]['generated_text'])\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "og4Ev8y2VeSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned LoRA adapters\n",
        "# --- Step 5: Save your fine-tuned model adapters locally ---\n",
        "# (This is the same as before, ensuring the files are ready)\n",
        "\n",
        "lora_model_name = \"medgemma-guidelines-4b-4bit-lora\"\n",
        "model.save_pretrained(lora_model_name)\n",
        "tokenizer.save_pretrained(lora_model_name)\n",
        "\n",
        "print(f\"LoRA adapters saved locally to '{lora_model_name}'\")\n",
        "\n",
        "\n",
        "# --- Step 6: Log in and Upload to Hugging Face Hub ---\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# 1. Log in to your Hugging Face account\n",
        "# A widget will appear. Paste your access token with 'write' permissions here.\n",
        "notebook_login()\n",
        "\n",
        "# 2. Push the model adapters to the Hub\n",
        "# The push_to_hub command will create a new repository if it doesn't exist.\n",
        "# Make sure to replace \"your-hf-username\" with your actual Hugging Face username.\n",
        "hf_repo_name = \"huseyincavus/medgemma-4b-guidelines-lora\"\n",
        "\n",
        "print(f\"Uploading adapters to Hugging Face Hub repository: {hf_repo_name}\")\n",
        "model.push_to_hub(hf_repo_name, use_auth_token=True)\n",
        "tokenizer.push_to_hub(hf_repo_name, use_auth_token=True)\n",
        "print(\"Upload complete!\")"
      ],
      "metadata": {
        "id": "8rtRTGuJVeu3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}